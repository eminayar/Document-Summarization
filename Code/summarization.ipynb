{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my assignment, I used following two models:\n",
    "    1. Word to sentence is implemented by simply averaging word vectors which are given\n",
    "    2. Word to sentence is implemented by weighted average where weight is derived from tf_idf values of words.\n",
    "I chose K as sqrt(N) where N is the number of sentences in a document.\n",
    "\n",
    "Whole code runs in 3 minutes in my local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "from rouge import Rouge\n",
    "\n",
    "VECTOR_PATH = '../pre_trained_word2vec_model'\n",
    "ARTICLES_PATH = '../articles/'\n",
    "GOLD_SUMMARIES_PATH = '../gold_summaries/'\n",
    "\n",
    "ALL_FILES = os.listdir(os.path.abspath(os.path.join( os.getcwd() , ARTICLES_PATH)))\n",
    "FILE_NAMES = []\n",
    "dic = {}\n",
    "docs_sentences = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code in the below cell is taken and edited from https://stackoverflow.com/questions/4576077/python-split-text-on-sentences. It is a rule based sentence finder using regex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabets= \"([A-Za-z])\"\n",
    "prefixes = \"(Mr|St|Mrs|Ms|Dr|Prof|Capt|Cpt|Lt|Mt|US)[.]\"\n",
    "suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "starters = \"(Mr|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "websites = \"[.](com|net|org|io|gov|me|edu)\"\n",
    "digits = \"([0-9])\"\n",
    "punctuations = {'#', '[', '~', '-', ']', '.', '@', '/', \"'\", '{', '|', ')',\n",
    "                '(', '*', ',', '`', ';', '$', '%', '\\\\', '^', '_', '!', '<', ':', '&', '>', '\"', '}', '=', '?', '+'}\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    if \"e.g.\" in text: text = text.replace(\"e.g.\",\"e<prd>g<prd>\")\n",
    "    if \"i.e.\" in text: text = text.replace(\"i.e.\",\"i<prd>e<prd>\")    \n",
    "    text = re.sub(digits + \"[.]\" + digits,\"\\\\1<prd>\\\\2\",text)\n",
    "    text = re.sub(\"\\s\" + alphabets + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(alphabets + \"[.]\" + alphabets + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + alphabets + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    if \"...\" in text: text = text.replace(\"...\",\"<prd><prd><prd>\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takes less than a minute. I simply read articles, then using above rule based splitter i found sentences. I returned a list of sentences for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_documents_to_sentences():\n",
    "    global FILE_NAMES\n",
    "    docs_by_sentences = []\n",
    "    for article_name in FILE_NAMES:\n",
    "        article = open( os.path.abspath( os.path.join( os.getcwd() , ARTICLES_PATH , article_name ) ) , \"r\" , encoding='latin1' )\n",
    "        sentences = [article.readline().strip()] \n",
    "        article = article.read()\n",
    "        sentences.extend( split_into_sentences( article ) )\n",
    "        docs_by_sentences.append( sentences )\n",
    "    return docs_by_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Takes a string, removes punctuations by replacing them with space, makes every word lowercase and returns the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize( input ):\n",
    "    input = input.strip()\n",
    "    without_punc = \"\"\n",
    "    for char in input:\n",
    "        if char in punctuations:\n",
    "            without_punc += ' '\n",
    "        else:\n",
    "            without_punc += char\n",
    "    return list(map(lambda x: x.lower(), without_punc.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the implementation for the first model, which takes average of the word vectors to represent a sentence. I also keep a global list named *docs_sentences* to keep the original sentences for the summaries. For safety, I consider the words which has no correspondence with the given pre-trained vector model as if they don't exist in the sentence. \n",
    "\n",
    "Takes less than a minute in my local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2vec(docs_by_sentences):\n",
    "    global FILE_NAMES\n",
    "    global docs_sentences\n",
    "    global dic\n",
    "    docs_as_vectors = []\n",
    "    for i in range(len(FILE_NAMES)):\n",
    "        sentences = []\n",
    "        actual_sentences = []\n",
    "        for sentence in docs_by_sentences[i]:\n",
    "            tokens = tokenize( sentence )\n",
    "            tokens = list(filter(lambda token: token in dic, tokens) )\n",
    "            if len(tokens) == 0:\n",
    "                continue\n",
    "            average = np.zeros(200)\n",
    "            for token in tokens:\n",
    "                average += np.array(dic[token])/len(tokens)\n",
    "            sentences.append(average)\n",
    "            actual_sentences.append(sentence)\n",
    "        docs_as_vectors.append( sentences )\n",
    "        docs_sentences[i] = actual_sentences\n",
    "    return docs_as_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the implementation for the second model, which takes the weighted average of the word vectors to represent a sentence where weights are related to their *tf_idf* values. First I calculate the frequency and inversed document frequencies of words. And then using these I construct a weight formula which is *(1+log(freq))X(log(N/df)+epsilon)*.\n",
    "Rest of the logic is the same as the first model.\n",
    "\n",
    "Takes less than a minute in my local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2vec2(docs_by_sentences):\n",
    "    global FILE_NAMES\n",
    "    global docs_sentences\n",
    "    global dic\n",
    "    vocabulary = {}\n",
    "    freq = [{} for i in range(len(FILE_NAMES))]\n",
    "    for doc_i in range(len(FILE_NAMES)):\n",
    "        doc = docs_by_sentences[doc_i]\n",
    "        all_tokens_of_a_doc = []\n",
    "        for sentence in doc:\n",
    "            tokens = tokenize( sentence )\n",
    "            tokens = list(filter(lambda token: token in dic, tokens) )\n",
    "            all_tokens_of_a_doc.extend(tokens)\n",
    "        for token in all_tokens_of_a_doc:\n",
    "            if token not in freq[doc_i]:\n",
    "                freq[doc_i][token] = 0\n",
    "            freq[doc_i][token] += 1/len(all_tokens_of_a_doc)\n",
    "        for token in set(all_tokens_of_a_doc):\n",
    "            if token not in vocabulary:\n",
    "                vocabulary[token] = 0\n",
    "            vocabulary[token] += 1\n",
    "\n",
    "    docs_as_vectors = []\n",
    "    for i in range(len(FILE_NAMES)):\n",
    "        sentences = []\n",
    "        actual_sentences = []\n",
    "        for sentence in docs_by_sentences[i]:\n",
    "            tokens = tokenize( sentence )\n",
    "            tokens = list(filter(lambda token: token in dic, tokens) )\n",
    "            if len(tokens) == 0:\n",
    "                continue\n",
    "            average = np.zeros(200)\n",
    "            weights = []\n",
    "            for token in tokens:\n",
    "                weights.append( (1+np.log(freq[i][token]))*(np.log(len(FILE_NAMES)/vocabulary[token]+0.000001) ))\n",
    "            weights = np.ndarray.tolist( np.array(weights)/np.sum(weights) )\n",
    "            for j in range(len(tokens)):\n",
    "                token = tokens[j]\n",
    "                average += np.array(dic[token])*weights[j]\n",
    "            sentences.append(average)\n",
    "            actual_sentences.append(sentence)\n",
    "        docs_as_vectors.append( sentences )\n",
    "        docs_sentences[i] = actual_sentences\n",
    "    return docs_as_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part I implemented the k-means clustering algorithm. I think it is not necessary to explain the algorithm because i sitrictly followed the algorithm which is tought us in the lecture. I chose K as square root of the number of sentences in the given document. I iterated the algorithm until the sum of the euclidian distances between the old and the new centroids are smaller than 10^-4.\n",
    "\n",
    "Then for each cluster, I found the sentence which is closest to the centroid of that cluster. I concatanated the strings and returned them.\n",
    "\n",
    "Takes around 2 minutes in my local."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster(docs_as_vectors):\n",
    "    global docs_sentences\n",
    "    extracted_summaries = []\n",
    "    for mask in range(len(docs_as_vectors)):\n",
    "        document = docs_as_vectors[mask]\n",
    "        num_of_sentences = len(document)\n",
    "        K = int(np.ceil(np.sqrt(num_of_sentences)))\n",
    "        centroids = np.arange(num_of_sentences)\n",
    "        np.random.shuffle( centroids)\n",
    "        centroids = [document[x] for x in centroids[:K]]\n",
    "        diff = 100\n",
    "        while diff > 0.0001:\n",
    "            clusters = []\n",
    "            num_of_elements = [0 for x in range(K)]\n",
    "            for sentence in document:\n",
    "                closest = -1\n",
    "                point = -1\n",
    "                for i in range(len(centroids)):\n",
    "                    dist = np.linalg.norm(sentence-centroids[i])\n",
    "                    if closest == -1 or dist < closest:\n",
    "                        closest = dist\n",
    "                        point = i\n",
    "                clusters.append( point )\n",
    "                num_of_elements[point] += 1\n",
    "            old = centroids\n",
    "            centroids = [np.zeros(200) for x in range(K)]\n",
    "            for i in range(len(document)):\n",
    "                centroids[clusters[i]] += document[i]/num_of_elements[clusters[i]]\n",
    "            diff = 0\n",
    "            for i in range(len(centroids)):\n",
    "                diff += np.linalg.norm(old[i]-centroids[i])\n",
    "        summary = \"\"\n",
    "        for cnt in range(len(centroids)):\n",
    "            centroid = centroids[cnt]\n",
    "            closest = -1\n",
    "            point = -1\n",
    "            for i in range(len(document)):\n",
    "                if clusters[i] != cnt:\n",
    "                    continue\n",
    "                dist = np.linalg.norm(document[i]-centroid)\n",
    "                if closest == -1 or closest > dist:\n",
    "                    closest = dist\n",
    "                    point = i\n",
    "            summary+= \" \" + docs_sentences[mask][point]\n",
    "        extracted_summaries.append(summary.strip())\n",
    "    return extracted_summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function simply reads the gold summaries and constructs a list which has the same order of documents as extracted summaries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_gold_summary():\n",
    "    gold_summaries = []\n",
    "    for article_name in FILE_NAMES:\n",
    "        summary = open( os.path.abspath( os.path.join( os.getcwd() , GOLD_SUMMARIES_PATH , article_name ) ) , \"r\" , encoding='latin1' )\n",
    "        gold_summaries.append(summary.read())\n",
    "    return gold_summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self explanatory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(gold_summaries, extracted_summaries):\n",
    "    evaluator = Rouge()\n",
    "    return evaluator.get_scores(extracted_summaries, gold_summaries, avg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the given vector model. Split all files into 10 folds. For each fold, trait it as it is the test set. Since I have no variable which I am training, I don't have validation, simply calculate the scores for each fold seperately. Then take their mean and standart deviation and print them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MEANS:             F          P          R\n",
      "Model1 - Rouge1 [0.52259394 0.63581531 0.45332874]\n",
      "Model1 - Rouge2 [0.39099371 0.51182168 0.32515498]\n",
      "Model1 - RougeL [0.47776924 0.62097933 0.44261067]\n",
      "Model2 - Rouge1 [0.5266946  0.63708816 0.45918922]\n",
      "Model2 - Rouge2 [0.39543555 0.51262996 0.33129495]\n",
      "Model2 - RougeL [0.48305029 0.62265397 0.4488084 ]\n",
      "STANDARD DEVIATIONS:\n",
      "Model1 - Rouge1 [0.01239938 0.01686186 0.0106851 ]\n",
      "Model1 - Rouge2 [0.01594904 0.02267762 0.01331539]\n",
      "Model1 - RougeL [0.0126181  0.01819383 0.01161763]\n",
      "Model2 - Rouge1 [0.00684145 0.00964196 0.00611342]\n",
      "Model2 - Rouge2 [0.0079818  0.01134958 0.00701511]\n",
      "Model2 - RougeL [0.00669358 0.00976949 0.00609237]\n"
     ]
    }
   ],
   "source": [
    "fileObject = open( os.path.abspath( os.path.join( os.getcwd() , VECTOR_PATH ) ) , \"rb\")\n",
    "dic = pickle.load(fileObject)\n",
    "np.random.shuffle(ALL_FILES)\n",
    "chunks = np.array_split(ALL_FILES,10)\n",
    "M1rouge = [[] for j in range(3)]\n",
    "M2rouge = [[] for j in range(3)]\n",
    "for chunk in chunks:\n",
    "    FILE_NAMES = chunk\n",
    "    docs_sentences = [[] for x in range(len(FILE_NAMES))]\n",
    "\n",
    "    ###MODEL1\n",
    "    docs_by_sentences = parse_documents_to_sentences()\n",
    "    docs_as_vectors = sent2vec(docs_by_sentences)\n",
    "    extracted_summaries = cluster(docs_as_vectors)\n",
    "    gold_summaries = parse_gold_summary()\n",
    "    performance = evaluation(gold_summaries, extracted_summaries)\n",
    "    M1rouge[0].append(list(performance['rouge-1'].values()))\n",
    "    M1rouge[1].append(list(performance['rouge-2'].values()))\n",
    "    M1rouge[2].append(list(performance['rouge-l'].values()))\n",
    "\n",
    "    ###MODEL2\n",
    "    docs_as_vectors2 = sent2vec2(docs_by_sentences)\n",
    "    extracted_summaries2 = cluster(docs_as_vectors2)\n",
    "    performance2 = evaluation(gold_summaries, extracted_summaries2)\n",
    "    M2rouge[0].append(list(performance2['rouge-1'].values()))\n",
    "    M2rouge[1].append(list(performance2['rouge-2'].values()))\n",
    "    M2rouge[2].append(list(performance2['rouge-l'].values()))\n",
    "\n",
    "print(\"MEANS:             F          P          R\")\n",
    "\n",
    "print( \"Model1 - Rouge1 \" + str(np.mean(M1rouge[0], axis=0)) )\n",
    "print( \"Model1 - Rouge2 \" + str(np.mean(M1rouge[1], axis=0)) )\n",
    "print( \"Model1 - RougeL \" + str(np.mean(M1rouge[2], axis=0)) )\n",
    "\n",
    "print( \"Model2 - Rouge1 \" + str(np.mean(M2rouge[0], axis=0)) )\n",
    "print( \"Model2 - Rouge2 \" + str(np.mean(M2rouge[1], axis=0)) )\n",
    "print( \"Model2 - RougeL \" + str(np.mean(M2rouge[2], axis=0)) )\n",
    "\n",
    "print(\"STANDARD DEVIATIONS:\")\n",
    "print( \"Model1 - Rouge1 \" + str(np.std(M1rouge[0], axis=0)) )\n",
    "print( \"Model1 - Rouge2 \" + str(np.std(M1rouge[1], axis=0)) )\n",
    "print( \"Model1 - RougeL \" + str(np.std(M1rouge[2], axis=0)) )\n",
    "\n",
    "print( \"Model2 - Rouge1 \" + str(np.std(M2rouge[0], axis=0)) )\n",
    "print( \"Model2 - Rouge2 \" + str(np.std(M2rouge[1], axis=0)) )\n",
    "print( \"Model2 - RougeL \" + str(np.std(M2rouge[2], axis=0)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclustion\n",
    "\n",
    "I was expecting a better improvement moving from model1 to model2. There is a really, really small improvement. I though considering words' tf_idf values would give a better sense to their sentence representation. After all, definitive words will be more dominant than less definitive words. That way semantically close sentences would be closer to each other. I thought the process as eliminating the noise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
